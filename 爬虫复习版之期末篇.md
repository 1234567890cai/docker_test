# 爬虫复习版之期末篇

##### 什么是爬虫？考察概念

```python
网络爬虫又称网络蜘蛛、网络机器人，是一种按照一定规则、自动请求万维网网站，并提取网络数据的程序和脚本。
```

##### 爬虫的分类 （划分点 有条理）

```python
1. 按照使用场景划分：爬虫可以分为通用爬虫和聚焦爬虫。
2. 按照爬取形式： 爬虫可以分为累计式爬虫和增量式爬虫。
3. 按照爬取数据的存在方式： 爬虫可以分为表层爬虫和深层爬虫。
```

##### 爬虫抓取网页的通用流程（注意顺序）

以图片为准

![image-20221216105111337](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216105111337.png)

![image-20221216105127344](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216105127344.png)

##### robots.txt文件须知

```python
须知1：robots.txt文件是搜素引擎访问网站时查看的第一个文件，它会限制网络爬虫的访问范围。
须知2：当一个网络爬虫访问一个站点时，它会先检查该站点根目录下是否存在robots.txt文件
```

##### 防爬虫应对策略

```python 
1.设置User-Agent
2.使用代理IP
3.降低访问频率
4.验证码限制
```

##### 为什么选择python做爬虫

```python
1.爬虫网页本身的接口，相比其他静态编程语言(如Java、c#、c++)，python 爬取网页文档的接口更简洁。python中有很多非常优秀的第三方包支持,如Requests 或者 Mechanize等。
2. 网页爬取后的处理。python中的Beautiful Soup 提供了简洁的文档处理功能，能用极短的代码完成大部分文档的处理。
3.开发效率高。因为根据爬虫的具体代码需要根据网站不同而修改，python 作为灵活的脚本语言特别适合这种任务
4.上手快。好学，简单。现有资料丰富。
```

##### 客户端Http请求格式

![image-20221216162701616](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162701616.png)

![image-20221216162742719](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162742719.png)



![image-20221216162750770](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162750770.png)

![image-20221216162801786](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162801786.png)

![image-20221216162827311](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162827311.png)

![image-20221216162834705](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162834705.png)

##### 服务端Http格式

![image-20221216162856173](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162856173.png)

![image-20221216162915311](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162915311.png)

![image-20221216162933842](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216162933842.png)

![image-20221216163000522](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163000522.png)



![image-20221216163014554](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163014554.png)

![image-20221216163312502](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163312502.png)

![image-20221216163048144](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163048144.png)



![image-20221216163056021](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163056021.png)

![image-20221216163106199](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163106199.png)

##### Post和Get 请求的区别

![image-20221216112218292](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216112218292.png)

##### 状态码的不同含义

![image-20221216112320524](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216112320524.png)

##### Http抓包工具Fidder  P34页

![image-20221216201114373](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216201114373.png)

![image-20221216201546866](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216201546866.png)





##### Urllib的常见方法

```python
1.geturl():获取响应内容的url
2.info():返回页面的原信息
3.getcode():返回http请求的响应状态码
url 编码转换
import urllib.parse 
编码：urllib.parse.urlencode()  #字典形式
解码：urllib.parse.unquote() 
```

Requests库的常见方法和属性

````python
属性
1.status_code          http请求的返回状态
2.text      Http响应内容的字符串形式
3.encoding     指定编码
4.content        Http响应内容的二进制形式
5.json()     json解码器
````

##### 正则表达式

```pyhon
区分三种方法
re.match() 匹配开头找不到为None 如果不满足直接报错
re.searh() 只匹配一次找不到返回None
re.findall()  找到所有符合情况的放入列表找不到为None
\d 匹配一个数字  \d+匹配多个数字  \D 匹配非数字 . 就是匹配任意字符除了\n
量词：*？+：
* ：匹配0个或者无数个
+ :表示匹配一次或者无数次
？：匹配0个或者1个
```

##### Xpath

```python
/ 表示从根节点选取
// 匹配任意位置
. 表示当前位置
.. 表示当前的父节点
@ 属性
text() 拿文本信息


```

##### Json 的定义和使用(85页)

```python
定义：是一种轻量级的数据交换格式，可使人们容易地进行阅读和编写，同时也方便了机器进行解析和生成。
JSON概述：
Json是比XML更简单的一种数据交互格式，它完全独立于编程语言的文本格式来存储和表示数据。
使用：
d开头的就是将 python类型-->json 类型
dumps():序列化
dump()：序列化
l开头的就是将json-->python 类型
load()：反序列化
loads()：反序列化
```

##### Mongodb 

```python
概念
MongoDB是一款由c++语言编写的，基于分布式文件存储的NoSQL数据库。
区分Mysql:
MongoDB:是一种非关系型数据库，它没有表的概念,其数据库的基本存成单位是集合。
基本操作：
第一步：先导包 
from pymongo import *
第二步 创建连接
client =Mongoclient('localhost',27017)
第三步 访问数据库
db=client.自定义 （如果数据库不存在会自己创建，存在就打开）
第四步 创建集合
column = db.集合名字
第五步 插入文档  /更新数据update_one()  update_many()  /查询find() find_one() /删除 delete_many()  delete_one()
insert_one()  插入一条 字典的形式
insert_many()  插入多条   列表套字典的形式
第六步  关闭连接
client.close()
```

##### 自动化程序 Selenium 

```python
定义：Selenium 是一个Web 自动化测试工具，最初是为网站自动化测试而开发的，可以按指定的命令自动操作
Selenium 库里面有一个Webdriver的API 。WebDriver类似于加载网站的浏览器，可以像Beautiful Soup或者其他的Selector对象一样来查找页面元素，与页面上的元素进行交互。
```

![image-20221216161133506](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216161133506.png)

![image-20221216161147574](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216161147574.png)

![image-20221216161444063](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216161444063.png)

##### Scrapy 框架 

```python
定义： Scrapy 是用纯python 实现的开源的爬虫框架。Scrapy使用了Twisted异步网络框架来处理网络通信，该网络框架可以加快下载速度，并且包含了各种中间件接口，可以灵活的完成各种需求。
组成部分
Scrapy Engine(引擎)      信号的传递
Scheduler(调度器)     接收引擎发过来的Request请求，按照一定方法排队入列
Downloader(下载器)  下载请求  响应还给引擎 由引擎交给爬虫
Spider(爬虫)       负责处理所有的响应,从中提取数据，获取Item想字段要的数据，并将需要跟进的url提交给引擎，再次进入调度器。
Item Pipeline(管道) 负责处理爬虫中获取到的ITEM数据，并进行后期处理
如果题目说的是5大组件就写上面的5个。
如果说7大组件再加上这两个
Downloader Middlewares(下载中间件)
Spider Middlewares(爬虫中间件)
各部分的功能
```

![image-20221216163903428](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163903428.png)

![image-20221216163944308](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216163944308.png)

![image-20221216164023555](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216164023555.png)

![image-20221216164131728](C:\Users\蔡梦丹\AppData\Roaming\Typora\typora-user-images\image-20221216164131728.png)

```python
命令
创建爬虫域
scrapy genspider 爬虫名称 "爬虫域"
永久性存储信息
scrapy crawl 爬虫名字 -o 文件名
这里的文件名可以是 json csv excel ....


```

